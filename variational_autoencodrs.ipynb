{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNe76paJKd10gH38Deatewp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yavuzuzun/machine-learning-practice/blob/main/variational_autoencodrs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Variational Auto-Encoders\n",
        "\n",
        "Variational Autoencoders (VAEs) are generative models that can learn the latent representation of high-dimensional data. VAEs are a type of artificial neural network that uses a probabilistic approach to encode and decode data. The main objective of VAEs is to learn the underlying structure of the data by modeling the probability distribution of the data in a low-dimensional latent space.\n",
        "\n",
        "VAEs are composed of two main parts: an encoder and a decoder. The encoder maps the input data to a low-dimensional latent space, which is a compressed representation of the input data. The decoder then maps the latent space back to the input data. The objective of the VAE is to optimize the encoding and decoding functions so that the reconstructed data is similar to the original input data.\n",
        "\n",
        "The encoder of a VAE typically consists of a series of neural network layers that map the input data to a latent space distribution. The distribution is typically Gaussian, with a mean and a variance. The mean and variance vectors are used to sample a random point from the latent space, which is then fed into the decoder.\n",
        "\n",
        "The decoder of a VAE is also typically composed of a series of neural network layers that map the latent space back to the input data space. The output of the decoder is the reconstructed input data.\n",
        "\n",
        "During the training phase, the VAE tries to minimize a loss function, which is typically the sum of two terms: the reconstruction loss, which measures the difference between the input data and the reconstructed data, and the KL divergence loss, which measures the difference between the latent space distribution and a unit Gaussian distribution.\n",
        "\n",
        "The VAE is a powerful generative model that can be used to generate new data samples by sampling from the learned latent space distribution. It has been successfully applied to a wide range of tasks, including image generation, anomaly detection, and data imputation."
      ],
      "metadata": {
        "id": "tHLkEGxFjUJH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# With prepackage"
      ],
      "metadata": {
        "id": "ZGcmXXwvjcSo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using Keras"
      ],
      "metadata": {
        "id": "Demti2-6yqBh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PvSg-DRCfoBp"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Define encoder\n",
        "latent_dim = 2  # Size of latent space\n",
        "\n",
        "encoder_inputs = keras.Input(shape=(28, 28, 1))\n",
        "x = layers.Conv2D(32, 3, activation=\"relu\", strides=2, padding=\"same\")(encoder_inputs)\n",
        "x = layers.Conv2D(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
        "x = layers.Flatten()(x)\n",
        "x = layers.Dense(16, activation=\"relu\")(x)\n",
        "z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
        "z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
        "\n",
        "# Reparameterization trick\n",
        "def sampling(args):\n",
        "    z_mean, z_log_var = args\n",
        "    epsilon = tf.keras.backend.random_normal(shape=(tf.keras.backend.shape(z_mean)[0], latent_dim),\n",
        "                                              mean=0.0, stddev=1.0)\n",
        "    return z_mean + tf.keras.backend.exp(0.5 * z_log_var) * epsilon\n",
        "\n",
        "z = layers.Lambda(sampling, output_shape=(latent_dim,), name=\"z\")([z_mean, z_log_var])\n",
        "\n",
        "encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
        "\n",
        "# Define decoder\n",
        "latent_inputs = keras.Input(shape=(latent_dim,))\n",
        "x = layers.Dense(7 * 7 * 64, activation=\"relu\")(latent_inputs)\n",
        "x = layers.Reshape((7, 7, 64))(x)\n",
        "x = layers.Conv2DTranspose(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
        "x = layers.Conv2DTranspose(32, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
        "decoder_outputs = layers.Conv2DTranspose(1, 3, activation=\"sigmoid\", padding=\"same\")(x)\n",
        "\n",
        "decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
        "\n",
        "# Define VAE\n",
        "class VAE(keras.Model):\n",
        "    def __init__(self, encoder, decoder, **kwargs):\n",
        "        super(VAE, self).__init__(**kwargs)\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def train_step(self, data):\n",
        "        if isinstance(data, tuple):\n",
        "            data = data[0]\n",
        "        with tf.GradientTape() as tape:\n",
        "            z_mean, z_log_var = self.encoder(data)\n",
        "            z = self.sampling((z_mean, z_log_var))\n",
        "            reconstruction = self.decoder(z)\n",
        "            reconstruction_loss = tf.reduce_mean(\n",
        "                keras.losses.binary_crossentropy(data, reconstruction)\n",
        "            )\n",
        "            reconstruction_loss *= 28 * 28\n",
        "            kl_loss = 1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)\n",
        "            kl_loss = tf.reduce_mean(kl_loss)\n",
        "            kl_loss *= -0.5\n",
        "            total_loss = reconstruction_loss + kl_loss\n",
        "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
        "        return {\n",
        "            \"loss\": total_loss,\n",
        "            \"reconstruction_loss\": reconstruction_loss,\n",
        "            \"kl_loss\": kl_loss,\n",
        "        }\n",
        "\n",
        "    def call(self, inputs):\n",
        "        z_mean, z_log_var = self.encoder(inputs)\n",
        "        z = self.sampling((z_mean, z_log_var))\n",
        "        return self.decoder(z)\n",
        "\n",
        "    def sampling(self, args):\n",
        "        z_mean, z_log_var = args\n",
        "        batch = tf.shape(z_mean)[0]\n",
        "        dim = tf.shape(z_mean)[1]\n",
        "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
        "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
        "\n",
        "# Instantiate VAE model\n",
        "vae = VAE(encoder, decoder)\n",
        "\n",
        "# Compile the model\n",
        "vae.compile(optimizer=keras.optimizers.Adam())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1 - Training the model:\n",
        "\n",
        "To train the VAE model, you can simply call the fit() method on the VAE object and pass in your training data. For example:"
      ],
      "metadata": {
        "id": "g7XNyn28rNi6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "# Load MNIST dataset\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "# Normalize images and reshape them\n",
        "train_images = train_images.astype(\"float32\") / 255.0\n",
        "test_images = test_images.astype(\"float32\") / 255.0\n",
        "train_images = np.reshape(train_images, (train_images.shape[0], 28, 28, 1))\n",
        "test_images = np.reshape(test_images, (test_images.shape[0], 28, 28, 1))\n",
        "\n",
        "# Train the model\n",
        "vae.fit(train_images, epochs=10, batch_size=128, validation_data=(test_images, None))\n"
      ],
      "metadata": {
        "id": "764wzHu5pdyH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 554
        },
        "outputId": "5b6d37ce-144f-425b-fdbb-b396628fc5f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-9d72f96ed811>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-ea4745376534>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mz_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_log_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m             \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msampling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_log_var\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mreconstruction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"<ipython-input-1-ea4745376534>\", line 49, in train_step\n        z_mean, z_log_var = self.encoder(data)\n\n    ValueError: too many values to unpack (expected 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This will train the VAE model on the MNIST dataset for 10 epochs with a batch size of 128.\n",
        "\n",
        "2 - Generating new images:\n",
        "\n",
        "To generate new images from the VAE model, you can sample points from the latent space and decode them into images using the decoder model. For example:"
      ],
      "metadata": {
        "id": "prG5OTy3rN8-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate random points in the latent space\n",
        "random_latent_vectors = np.random.normal(size=(10, latent_dim))\n",
        "\n",
        "# Decode the random latent vectors into images\n",
        "generated_images = decoder.predict(random_latent_vectors)\n",
        "\n",
        "# Plot the generated images\n",
        "for i in range(generated_images.shape[0]):\n",
        "    plt.subplot(2, 5, i+1)\n",
        "    plt.imshow(generated_images[i, :, :, 0], cmap='gray')\n",
        "    plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OwaVBbzbrUZ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This will generate 10 random points in the latent space, decode them into images, and plot them.\n",
        "\n",
        "3 - Encoding existing images:\n",
        "\n",
        "To encode existing images into their latent representations using the encoder model, you can simply call the predict() method on the encoder model and pass in the images. For example:"
      ],
      "metadata": {
        "id": "KBxPIB2IrpvA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode the test images into their latent representations\n",
        "z_mean, _, _ = encoder.predict(test_images)\n",
        "\n",
        "# Plot the latent representations\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.scatter(z_mean[:, 0], z_mean[:, 1], c=np.argmax(test_labels, axis=1), cmap='viridis')\n",
        "plt.colorbar()\n",
        "plt.xlabel('z[0]')\n",
        "plt.ylabel('z[1]')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FY9eN0JartVO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This will encode the test images into their latent representations using the encoder model, and plot the latent representations in a 2D scatter plot. The color of each point corresponds to the label of the corresponding image."
      ],
      "metadata": {
        "id": "2c0y9Bsxry5u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using PyTorch"
      ],
      "metadata": {
        "id": "zBPGOyHcyGBB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define encoder\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, latent_dim):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1)\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(7*7*64, 16)\n",
        "        self.fc_mean = nn.Linear(16, latent_dim)\n",
        "        self.fc_log_var = nn.Linear(16, latent_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.conv1(x))\n",
        "        x = torch.relu(self.conv2(x))\n",
        "        x = self.flatten(x)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        mean = self.fc_mean(x)\n",
        "        log_var = self.fc_log_var(x)\n",
        "        return mean, log_var\n",
        "\n",
        "# Define decoder\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, latent_dim):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(latent_dim, 7*7*64)\n",
        "        self.reshape = nn.Unflatten(-1, (64, 7, 7))\n",
        "        self.conv1 = nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
        "        self.conv2 = nn.ConvTranspose2d(32, 1, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.reshape(x)\n",
        "        x = torch.relu(self.conv1(x))\n",
        "        x = torch.sigmoid(self.conv2(x))\n",
        "        return x\n",
        "\n",
        "# Define VAE\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, latent_dim):\n",
        "        super(VAE, self).__init__()\n",
        "\n",
        "        self.latent_dim = latent_dim\n",
        "        self.encoder = Encoder(latent_dim)\n",
        "        self.decoder = Decoder(latent_dim)\n",
        "\n",
        "    def encode(self, x):\n",
        "        mean, log_var = self.encoder(x)\n",
        "        return mean, log_var\n",
        "\n",
        "    def reparameterize(self, mean, log_var):\n",
        "        std = torch.exp(0.5 * log_var)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mean + eps * std\n",
        "\n",
        "    def decode(self, z):\n",
        "        return self.decoder(z)\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean, log_var = self.encode(x)\n",
        "        z = self.reparameterize(mean, log_var)\n",
        "        return self.decode(z), mean, log_var\n",
        "\n",
        "    def loss_function(self, recon_x, x, mu, logvar):\n",
        "        BCE = nn.functional.binary_cross_entropy(recon_x.view(-1, 784), x.view(-1, 784), reduction='sum')\n",
        "        KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "        return BCE + KLD\n",
        "\n",
        "# Instantiate VAE model\n",
        "vae = VAE(latent_dim=2)\n",
        "\n",
        "# Define optimizer\n",
        "optimizer = torch.optim.Adam(vae.parameters())"
      ],
      "metadata": {
        "id": "ako-3MAhyEdm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To train the model, you will need a dataset of images to feed into the VAE. Here is an example of how you could train the VAE on the MNIST dataset:"
      ],
      "metadata": {
        "id": "f3VYiNAHyfff"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.datasets as dsets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Define dataset and dataloader\n",
        "batch_size = 128\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "train_dataset = dsets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Instantiate VAE model\n",
        "vae = VAE(latent_dim=2)\n",
        "\n",
        "# Define optimizer\n",
        "optimizer = torch.optim.Adam(vae.parameters())\n",
        "\n",
        "# Training loop\n",
        "epochs = 10\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "    for batch_idx, (x, _) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        recon_x, mu, log_var = vae(x)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = vae.loss_function(recon_x, x, mu, log_var)\n",
        "\n",
        "        # Backward pass and update weights\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader.dataset)\n",
        "    print(f'Epoch {epoch+1}/{epochs}, Average Loss: {avg_loss:.4f}')"
      ],
      "metadata": {
        "id": "2fjYAMtoyEa7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, we first define a dataset and dataloader for the MNIST dataset using PyTorch's DataLoader class. We then instantiate the VAE model and define an optimizer (Adam in this case). Finally, we loop through the dataset for a specified number of epochs, computing the loss and updating the weights after each batch. The loss function used is the sum of the reconstruction loss and the Kullback-Leibler divergence (KLD) between the learned distribution and a unit Gaussian. The KLD term encourages the learned distribution to be close to a unit Gaussian, which helps prevent overfitting."
      ],
      "metadata": {
        "id": "VuFGQZRAymxJ"
      }
    }
  ]
}